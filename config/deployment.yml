# Deployment Configuration for Stage 2
# Docker Compose stack: vLLM server + FastAPI gateway
# Critical values are stored in AWS SSM Parameter Store
# Static configuration values are defined here as defaults

aws:
  region: /slm-ft-serving/aws/region  # SSM Parameter
  
ec2:
  instance_id: /slm-ft-serving/ec2/instance-id  # SSM Parameter
  instance_type: g6.2xlarge
  
ecr:
  # Separate repositories for vLLM and gateway images
  vllm_repository: slm-ft-serving-vllm
  gateway_repository: slm-ft-serving-gateway
  # ECR registry will be dynamically constructed from account_id and region

secrets:
  # HuggingFace token stored in Secrets Manager, referenced via SSM parameter
  hf_token: /slm-ft-serving/secrets/hf-token  # SSM Parameter

model:
  base_model: meta-llama/Llama-3.1-8B
  adapter_model: loghoag/llama-3.1-8b-medical-ie

vllm:
  api_port: 8000
  tensor_parallel_size: 1  # g6.2xlarge has 1 GPU

gateway:
  api_port: 8080
  cors_origins: "*"  # Stage 2: Allow all for testing, restrict in Stage 3
  
docker:
  # Container names managed by docker-compose (vllm-server, fastapi-gateway)
  # Named volume persists models on EBS across container recreations
  volume_name: huggingface-cache
  # Mount point inside container where vLLM caches models
  cache_mount_path: /root/.cache/huggingface
  
cloudwatch:
  log_group_ssm: /aws/ssm/slm-ft-serving/commands
  log_group_deployment: /aws/deployment/slm-ft-serving
  
deployment:
  max_retries: 3
  retry_delay_seconds: 10
  ec2_start_timeout_seconds: 300
  ssm_command_timeout_seconds: 600
  health_check_timeout_seconds: 300  # 5 minutes for vLLM model loading
  health_check_interval_seconds: 10
