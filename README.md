# Medical Information Extraction - Backend Server

> **Backend infrastructure for AI-powered medical cancer information extraction using fine-tuned Llama 3.1 8B**

[![Live Demo](https://img.shields.io/badge/Demo-Live-brightgreen)](https://medical-extraction.vercel.app)
[![GitHub Actions](https://img.shields.io/badge/CI/CD-GitHub%20Actions-2088FF?logo=github-actions&logoColor=white)](https://github.com/longhoag/slm-ft-serving/actions)
[![AWS](https://img.shields.io/badge/AWS-EC2%20%7C%20ECR%20%7C%20SSM-FF9900?logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Docker](https://img.shields.io/badge/Docker-vLLM%20%7C%20FastAPI-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)

---

## ğŸ¯ Overview

This repository contains the **backend infrastructure** for a medical information extraction system that uses a **fine-tuned Llama 3.1 8B model** (qLoRA 4-bit quantization) to extract structured cancer-related entities from clinical text.

The system serves as an AI-powered medical assistant that can parse unstructured clinical notes and return structured data including cancer type, stage, gene mutations, biomarkers, treatments, responses, and metastasis sites.

**ğŸŒ Live Application**: [https://medical-extraction.vercel.app](https://medical-extraction.vercel.app)

**ğŸ“± Frontend Repository**: [slm-ft-serving-frontend](https://github.com/longhoag/slm-ft-serving-frontend)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Browser                           â”‚
â”‚                (medical-extraction.vercel.app)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTPS
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Next.js Frontend (Vercel)                    â”‚
â”‚  â€¢ React UI components                                        â”‚
â”‚  â€¢ Server-side API routes (proxy to backend)                  â”‚
â”‚  â€¢ Input validation & error handling                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP (proxied)
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  EC2 g6.2xlarge (us-east-1)                   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  FastAPI Gateway (Port 8080)                            â”‚  â”‚
â”‚  â”‚  â€¢ REST API endpoints                                   â”‚  â”‚
â”‚  â”‚  â€¢ Request validation (Pydantic)                        â”‚  â”‚
â”‚  â”‚  â€¢ CORS configuration                                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚ HTTP                              â”‚
â”‚                           â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  vLLM Server (Port 8000)                                â”‚  â”‚
â”‚  â”‚  â€¢ Llama 3.1 8B base model                              â”‚  â”‚
â”‚  â”‚  â€¢ LoRA adapter: medical-ie                             â”‚  â”‚
â”‚  â”‚  â€¢ GPU inference (L4 GPU)                               â”‚  â”‚
â”‚  â”‚  â€¢ Model caching on EBS                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> âš ï¸ **Note**: This is an experimental project. The EC2 backend server may not be running at all times to save costs (~$1/hour for GPU instance). If the demo is unavailable, the server is likely stopped.

### Component Details

| Component | Technology | Port | Description |
|-----------|-----------|------|-------------|
| **Frontend** | Next.js 16 + React | N/A | User interface on Vercel (separate repo) |
| **Gateway** | FastAPI + Pydantic | 8080 | REST API layer with validation |
| **Inference** | vLLM + Llama 3.1 8B | 8000 | LLM serving with LoRA adapter |
| **Infrastructure** | EC2 + Docker Compose | N/A | Container orchestration |
| **CI/CD** | GitHub Actions + ECR | N/A | Automated builds and deployments |

---

## ğŸš€ Project Stages

This project follows a **staged development approach** - each stage must be complete before moving to the next:

| Stage | Status | Description | Documentation |
|-------|--------|-------------|---------------|
| **1** | âœ… Complete | vLLM server with LoRA adapter on EC2 | [Stage 1 Details](#stage-1-vllm-inference-server) |
| **2** | âœ… Complete | FastAPI gateway with Docker Compose | [Stage 2 Details](#stage-2-fastapi-gateway) |
| **3** | âœ… Complete | Next.js frontend on Vercel | [Stage 3 Details](#stage-3-nextjs-frontend) |
| **4** | ğŸ”® Planned | CloudWatch monitoring & observability | [Stage 4 Preview](#stage-4-future-monitoring) |

---

## ğŸ§¬ The Model

### Fine-tuning Details

The model was fine-tuned on synthetic cancer clinical data using **qLoRA (4-bit quantization)** technique for parameter-efficient training.

**Base Model**: [`meta-llama/Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B)  
**Fine-tuned Adapter**: [`loghoag/llama-3.1-8b-medical-ie`](https://huggingface.co/loghoag/llama-3.1-8b-medical-ie)

### Training Data Format

```json
{
  "instruction": "Extract all cancer-related entities from the text.",
  "input": "70-year-old man with widely metastatic cutaneous melanoma. PD-L1 was 5% on IHC and NGS reported TMB-high. Given multiple symptomatic brain metastases he received combination immunotherapy with nivolumab plus ipilimumab and stereotactic radiosurgery to dominant intracranial lesions. Imaging after two cycles demonstrated some shrinking of index lesions but appearance of a new small lesion â€” overall assessment called a mixed response.",
  "output": {
    "cancer_type": "melanoma (cutaneous)",
    "stage": "IV",
    "gene_mutation": null,
    "biomarker": "PD-L1 5%; TMB-high",
    "treatment": "nivolumab and ipilimumab; stereotactic radiosurgery",
    "response": "mixed response",
    "metastasis_site": "brain"
  }
}
```

### Extraction Fields

The model extracts **7 structured fields**:

| Field | Description | Example |
|-------|-------------|---------|
| `cancer_type` | Type of cancer | melanoma, breast cancer, NSCLC |
| `stage` | Cancer stage | III, IV, metastatic |
| `gene_mutation` | Genetic mutations | EGFR exon 19, KRAS G12D, BRCA1 |
| `biomarker` | Biomarker status | HER2+, PD-L1 5%, TMB-high |
| `treatment` | Treatments given | nivolumab, chemotherapy, surgery |
| `response` | Treatment response | complete response, stable disease |
| `metastasis_site` | Metastasis locations | brain, liver, bone |

---

## ğŸ› ï¸ Tech Stack

### Backend (This Repository)

| Category | Technology |
|----------|------------|
| **Inference Engine** | [vLLM](https://github.com/vllm-project/vllm) (optimized LLM serving) |
| **API Framework** | [FastAPI](https://fastapi.tiangolo.com/) + [Pydantic](https://docs.pydantic.dev/) |
| **Language** | Python 3.11+ |
| **Dependency Management** | Poetry |
| **Containerization** | Docker + Docker Compose |
| **Cloud Infrastructure** | AWS EC2 (g6.2xlarge, L4 GPU) |
| **Container Registry** | AWS ECR |
| **Remote Execution** | AWS Systems Manager (SSM) |
| **Secrets Management** | AWS Secrets Manager + SSM Parameter Store |
| **CI/CD** | GitHub Actions |
| **Logging** | Loguru + CloudWatch Logs |

### Frontend ([Separate Repository](https://github.com/longhoag/slm-ft-serving-frontend))

| Category | Technology |
|----------|------------|
| **Framework** | Next.js 16 (App Router) |
| **Language** | TypeScript (strict mode) |
| **Styling** | TailwindCSS v4 |
| **UI Components** | ShadcnUI (Radix primitives) |
| **Deployment** | Vercel (serverless) |
| **State Management** | React Hooks |


---

## ğŸ”§ Backend Deep Dive

### vLLM Inference Server

The core inference engine uses [vLLM](https://github.com/vllm-project/vllm) for high-performance LLM serving with optimized GPU utilization.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    vLLM Server (Port 8000)                  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Base Model     â”‚    â”‚  LoRA Adapter                   â”‚ â”‚
â”‚  â”‚  Llama 3.1 8B   â”‚â”€â”€â”€â–¶â”‚  medical-ie (71.8 MB)           â”‚ â”‚
â”‚  â”‚  (32.1 GB)      â”‚    â”‚  Fine-tuned for cancer IE       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                                 â”‚
â”‚           â–¼                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  NVIDIA L4 GPU (24GB VRAM)                              â”‚â”‚
â”‚  â”‚  â€¢ Continuous batching for throughput                   â”‚â”‚
â”‚  â”‚  â€¢ PagedAttention for memory efficiency                 â”‚â”‚
â”‚  â”‚  â€¢ OpenAI-compatible API (/v1/chat/completions)         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                             â”‚
â”‚  Model Cache: Docker volume on EBS (persists across runs)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features**:
- **LoRA Hot-Loading**: Adapter loaded at runtime without modifying base model
- **Model Persistence**: HuggingFace cache stored in Docker named volume
- **Health Endpoint**: `/health` returns status for container orchestration
- **Chat Template**: Custom Jinja template for instruction-following format

### FastAPI Gateway

Lightweight API layer that handles request validation, CORS, and response formatting.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FastAPI Gateway (Port 8080)                â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  /health     â”‚  â”‚  /docs       â”‚  â”‚  /api/v1/extract â”‚  â”‚
â”‚  â”‚  Health checkâ”‚  â”‚  Swagger UI  â”‚  â”‚  Main endpoint   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                               â”‚            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Request Processing Pipeline                         â”‚  â”‚
â”‚  â”‚  1. Pydantic validation (text, temperature, tokens)  â”‚  â”‚
â”‚  â”‚  2. Prompt construction (instruction + input format) â”‚  â”‚
â”‚  â”‚  3. vLLM API call (OpenAI-compatible)                â”‚  â”‚
â”‚  â”‚  4. JSON parsing of model output                     â”‚  â”‚
â”‚  â”‚  5. Structured response (7 medical fields)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  CORS: Restricted to Vercel domains (production + preview) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**API Endpoints**:

```bash
# Health check
GET /health
Response: {"status": "healthy", "vllm_available": true, "version": "0.1.0"}

# Medical extraction
POST /api/v1/extract
Content-Type: application/json
Body: {
  "text": "Patient diagnosed with stage 3 breast cancer...",
  "temperature": 0.3,  // optional: 0.0-2.0
  "max_tokens": 512    // optional: 1-8192
}
```

### Container Orchestration

Docker Compose manages the multi-container deployment with health check dependencies.

```yaml
# Simplified docker-compose.yml structure
services:
  vllm:
    image: ${ECR_REGISTRY}/slm-ft-serving-vllm:latest
    ports: ["8000:8000"]
    volumes: [huggingface-cache:/root/.cache/huggingface]
    deploy:
      resources:
        reservations:
          devices: [driver: nvidia, count: all, capabilities: [gpu]]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      start_period: 360s  # 6 min for model loading

  gateway:
    image: ${ECR_REGISTRY}/slm-ft-serving-gateway:latest
    ports: ["8080:8080"]
    environment: [VLLM_BASE_URL=http://vllm:8000]
    depends_on:
      vllm:
        condition: service_healthy  # Wait for vLLM ready
```

**Container Communication**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network (slm-network)             â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP (internal)    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚   Gateway   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â”‚    vLLM     â”‚    â”‚
â”‚   â”‚  :8080      â”‚   vllm:8000           â”‚   :8000     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚          â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ External (host network)
           â–¼
    Client requests to EC2:8080
```

### CI/CD Pipeline

GitHub Actions automates Docker image builds with parallel execution and ECR caching.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GitHub Actions Workflow                    â”‚
â”‚                  (Triggered on push to main)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                               â”‚
          â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build vLLM Image   â”‚         â”‚  Build Gateway Imageâ”‚
â”‚  (~2 min with cache)â”‚         â”‚  (~1 min with cache)â”‚
â”‚                     â”‚         â”‚                     â”‚
â”‚  â€¢ Free disk space  â”‚         â”‚  â€¢ Free disk space  â”‚
â”‚  â€¢ ECR login        â”‚         â”‚  â€¢ ECR login        â”‚
â”‚  â€¢ Docker buildx    â”‚         â”‚  â€¢ Docker buildx    â”‚
â”‚  â€¢ Push to ECR      â”‚         â”‚  â€¢ Push to ECR      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  AWS ECR Repositories   â”‚
              â”‚                         â”‚
              â”‚  slm-ft-serving-vllm    â”‚
              â”‚  slm-ft-serving-gateway â”‚
              â”‚                         â”‚
              â”‚  Cache: :buildcache tag â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Manual Deploy via SSM  â”‚
              â”‚  poetry run python      â”‚
              â”‚  scripts/deploy.py      â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Build Optimizations**:
- **Parallel Jobs**: vLLM and Gateway build simultaneously
- **ECR Registry Cache**: `--cache-from` / `--cache-to` for layer reuse
- **Disk Cleanup**: Remove unused tools before large builds
- **Minimal Tags**: Only `:latest` pushed to minimize storage costs

### Remote Deployment (SSM)

All EC2 operations execute via AWS Systems Manager - no SSH keys required.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     boto3/SSM API     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local Mac   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚  AWS SSM             â”‚
â”‚              â”‚                       â”‚                      â”‚
â”‚  deploy.py   â”‚                       â”‚  Run Command         â”‚
â”‚  â€¢ Start EC2 â”‚                       â”‚  â€¢ ECR login         â”‚
â”‚  â€¢ Wait OK   â”‚                       â”‚  â€¢ Pull images       â”‚
â”‚  â€¢ Send cmds â”‚                       â”‚  â€¢ docker compose up â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚  EC2 g6.2xlarge      â”‚
                                       â”‚                      â”‚
                                       â”‚  SSM Agent           â”‚
                                       â”‚  â”œâ”€ Fetch secrets    â”‚
                                       â”‚  â”œâ”€ Pull from ECR    â”‚
                                       â”‚  â””â”€ Start containers â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Security Model**:
- **No SSH**: Instance has no `.pem` key access
- **Secrets Manager**: HF token stored securely, fetched at runtime
- **SSM Parameter Store**: Configuration references (instance ID, secret names)

---

## ğŸš€ Getting Started

### Prerequisites

- AWS account with EC2, ECR, SSM, Secrets Manager access
- GitHub account with Actions enabled
- Poetry installed locally (`brew install poetry`)
- AWS CLI configured with credentials
- HuggingFace account with Llama 3.1 access

### Required Secrets

Store these in **AWS Secrets Manager**:

| Secret | Purpose | Location |
|--------|---------|----------|
| `HF_TOKEN` | HuggingFace access token | Secrets Manager |
| `AWS_ACCESS_KEY_ID` | AWS credentials | GitHub Secrets |
| `AWS_SECRET_ACCESS_KEY` | AWS credentials | GitHub Secrets |

Reference secrets via **SSM Parameter Store** (no `.env` files).

### Deployment

**1. Initial Setup**

```bash
# Clone repository
git clone https://github.com/longhoag/slm-ft-serving.git
cd slm-ft-serving

# Install dependencies
poetry install

# Configure AWS credentials
aws configure
```

**2. Deploy to EC2**

```bash
# Start EC2 instance and deploy containers
poetry run python scripts/deploy.py

# Deploy without starting EC2 (if already running)
poetry run python scripts/deploy.py --skip-start
```

**3. Verify Deployment**

```bash
# Check health
curl http://<ec2-public-ip>:8080/health

# Test extraction
curl -X POST http://<ec2-public-ip>:8080/api/v1/extract \
  -H "Content-Type: application/json" \
  -d '{"text": "Patient diagnosed with stage 3 breast cancer with HER2 positive marker."}'
```

### CI/CD Workflow

The GitHub Actions workflow automatically:

1. **Triggers on push** to `main` branch (when backend files change)
2. **Builds Docker images** (vLLM + Gateway in parallel)
3. **Pushes to ECR** with cache optimization
4. **Tracks deployment** in GitHub sidebar

**Manual deployment** to EC2:

```bash
poetry run python scripts/deploy.py --skip-start
```

---

## ğŸ“Š Features & Capabilities

### Backend Features

- âœ… **High-Performance Inference** - vLLM optimizations for fast LLM serving
- âœ… **GPU Acceleration** - NVIDIA L4 GPU for efficient inference
- âœ… **LoRA Adapter Support** - Load fine-tuned adapters without full model retraining
- âœ… **Model Caching** - Persistent storage on EBS (survives container restarts)
- âœ… **Health Checks** - Automated container health monitoring
- âœ… **Input Validation** - Pydantic models for request/response validation
- âœ… **CORS Security** - Restricted to Vercel domains
- âœ… **API Documentation** - Interactive Swagger UI at `/docs`
- âœ… **Structured Output** - 7 medical fields in JSON format
- âœ… **Error Handling** - Proper HTTP status codes and error messages

### Frontend Features ([View Frontend Repo](https://github.com/longhoag/slm-ft-serving-frontend))

- âœ¨ **Real-time Extraction** - Extract medical entities in 2-3 seconds
- ğŸ”’ **Secure Architecture** - EC2 backend IP hidden via server-side proxy
- ğŸ“± **Responsive Design** - Works seamlessly on mobile and desktop
- ğŸ¯ **Type-safe** - Full TypeScript coverage with strict mode
- âš¡ **Fast & Modern** - Built with Next.js 16 and TailwindCSS v4
- ğŸ”„ **Auto-deploy** - Push to main â†’ live on Vercel instantly

---

## ğŸ”§ Development Notes

### Design Principles

- **SSM-only access**: No SSH, no `.pem` keys for EC2 access
- **Secrets Manager**: All secrets stored securely, never in `.env` files
- **Poetry for Python**: No raw `pip install` commands
- **Loguru for logging**: No `print()` statements in production code
- **Staged development**: Complete each stage before moving forward
- **Fail-safe execution**: Commands execute with error handling and retries

### Project Structure

```
slm-ft-serving/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ deploy.yml              # CI/CD pipeline
â”‚   â””â”€â”€ copilot-instructions.md     # AI assistant context
â”œâ”€â”€ config/
â”‚   â””â”€â”€ deployment.yml              # Deployment configuration
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ STAGE-3.md                  # Stage 3 documentation
â”œâ”€â”€ gateway/
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ extraction.py           # Extraction endpoint
â”‚   â”œâ”€â”€ main.py                     # FastAPI app
â”‚   â””â”€â”€ Dockerfile                  # Gateway Docker image
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ deploy.py                   # SSM deployment script
â”œâ”€â”€ Dockerfile                      # vLLM Docker image
â”œâ”€â”€ docker-compose.yml              # Container orchestration
â”œâ”€â”€ pyproject.toml                  # Poetry dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸŒ Related Links

- **Live Application**: [https://medical-extraction.vercel.app](https://medical-extraction.vercel.app)
- **Frontend Repository**: [slm-ft-serving-frontend](https://github.com/longhoag/slm-ft-serving-frontend)
- **Base Model**: [meta-llama/Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B)
- **Fine-tuned Adapter**: [loghoag/llama-3.1-8b-medical-ie](https://huggingface.co/loghoag/llama-3.1-8b-medical-ie)

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **vLLM Team** - High-performance LLM inference engine
- **Meta AI** - Llama 3.1 base model
- **HuggingFace** - Model hosting and fine-tuning infrastructure
- **FastAPI** - Modern Python web framework
- **Vercel** - Frontend hosting platform
